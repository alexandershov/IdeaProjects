{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Langchain",
   "id": "d744cd36922ee32a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Langchain is a bullshit framework for developing LLM applications.\n",
    "\n",
    "It provides no real value over directly using LLMs API.\n",
    "\n",
    "It adds a crazy amount of abstraction (2001 java-like amount) for what essentially is function composition, f-strings, and collection of prompts.\n",
    "\n",
    "Code becomes hard to debug (and they sell debugging tool called LangSmith), everything is indirect.\n",
    "\n",
    "In theory, langchain makes switching between different LLMs easy, but good luck with that."
   ],
   "id": "199055fd18320047"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": 38,
   "source": "import langchain",
   "id": "initial_id"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-03T17:25:50.618255Z",
     "start_time": "2024-06-03T17:25:46.760332Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = getpass.getpass(\n",
    "    'enter openai api key, you can get it from https://platform.openai.com/api-keys')\n",
    "\n"
   ],
   "id": "7de79e49d3c78664",
   "outputs": [],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-03T17:25:57.863488Z",
     "start_time": "2024-06-03T17:25:50.619936Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(model='gpt-4o')\n",
    "\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"answer the questions as if you're Eminem\"),\n",
    "    HumanMessage(content=\"how to reverse a string in Python?\"),\n",
    "]\n",
    "\n",
    "response = model.invoke(messages)\n",
    "print(response)"
   ],
   "id": "a30452255143811d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Sure, reversing a string in Python is pretty simple. You can use slicing to achieve that. Here\\'s how you do it:\\n\\n```python\\noriginal_string = \"your_string_here\"\\nreversed_string = original_string[::-1]\\nprint(reversed_string)\\n```\\n\\nSo if you had, like, a string \"Eminem\", and you wanted to reverse it, it\\'d go something like this:\\n\\n```python\\noriginal_string = \"Eminem\"\\nreversed_string = original_string[::-1]\\nprint(reversed_string)  # Output will be \"menimE\"\\n```\\n\\nThat\\'s it. Quick and easy, just like dropping bars.' response_metadata={'token_usage': {'completion_tokens': 128, 'prompt_tokens': 27, 'total_tokens': 155}, 'model_name': 'gpt-4o', 'system_fingerprint': 'fp_319be4768e', 'finish_reason': 'stop', 'logprobs': None} id='run-446cb159-405a-4b97-bbe9-cc268fe29ead-0' usage_metadata={'input_tokens': 27, 'output_tokens': 128, 'total_tokens': 155}\n"
     ]
    }
   ],
   "execution_count": 40
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "`model.invoke` returns an instance of `AIMessage`\n",
    "with `StrOutputParser` we can convert it to string"
   ],
   "id": "67c78053f106a69a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-03T17:25:57.871840Z",
     "start_time": "2024-06-03T17:25:57.865990Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "parser = StrOutputParser()\n",
    "parser.invoke(response)"
   ],
   "id": "e83f6ecea978db09",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sure, reversing a string in Python is pretty simple. You can use slicing to achieve that. Here\\'s how you do it:\\n\\n```python\\noriginal_string = \"your_string_here\"\\nreversed_string = original_string[::-1]\\nprint(reversed_string)\\n```\\n\\nSo if you had, like, a string \"Eminem\", and you wanted to reverse it, it\\'d go something like this:\\n\\n```python\\noriginal_string = \"Eminem\"\\nreversed_string = original_string[::-1]\\nprint(reversed_string)  # Output will be \"menimE\"\\n```\\n\\nThat\\'s it. Quick and easy, just like dropping bars.'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 41
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Or we can create a chain by using `|` operator",
   "id": "d05a810b3a41ce25"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-03T17:25:57.876296Z",
     "start_time": "2024-06-03T17:25:57.873523Z"
    }
   },
   "cell_type": "code",
   "source": "chain = model | parser",
   "id": "fe27d9ed698f0254",
   "outputs": [],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-03T17:26:00.313233Z",
     "start_time": "2024-06-03T17:25:57.878195Z"
    }
   },
   "cell_type": "code",
   "source": "chain.invoke(messages)",
   "id": "1bee8877417a0a47",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Alright, so you wanna reverse a string in Python, huh? Here\\'s how you do it:\\n\\n```python\\nyour_string = \"reverse this\"\\nreversed_string = your_string[::-1]\\nprint(reversed_string)\\n```\\n\\nThat slice notation `[::-1]` is like flipping the script on the whole string, taking it from end to start. Easy as that.'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 43
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We can create `messages` from a template",
   "id": "f46fdcc34604c761"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-03T17:26:00.316523Z",
     "start_time": "2024-06-03T17:26:00.314039Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "system_template = \"answer the questions as if you're {character}\"\n",
    "user_template = \"{text}\"\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_template), (\"user\", user_template)])"
   ],
   "id": "d2fa012a54c0a7f6",
   "outputs": [],
   "execution_count": 44
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now we can use `prompt_template` to create prompts",
   "id": "d2124e8367bc95ad"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-03T17:26:00.320115Z",
     "start_time": "2024-06-03T17:26:00.317277Z"
    }
   },
   "cell_type": "code",
   "source": [
    "prompt_value = prompt_template.invoke(\n",
    "    {\n",
    "        \"character\": \"Taylor Swift\",\n",
    "        \"text\": \"how to reverse a string in Python?\"\n",
    "    })\n",
    "prompt_value"
   ],
   "id": "87e8f1b5be354fb5",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content=\"answer the questions as if you're Taylor Swift\"), HumanMessage(content='how to reverse a string in Python?')])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 45
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "As you can see it's just a simple string substitution and list generation",
   "id": "e4e503ea122daf3c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-03T17:26:00.322693Z",
     "start_time": "2024-06-03T17:26:00.320805Z"
    }
   },
   "cell_type": "code",
   "source": "chain = prompt_template | model | parser",
   "id": "ea5ed6b27e83581b",
   "outputs": [],
   "execution_count": 46
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-03T17:26:02.777028Z",
     "start_time": "2024-06-03T17:26:00.323526Z"
    }
   },
   "cell_type": "code",
   "source": [
    "chain.invoke(\n",
    "    {\n",
    "        \"character\": \"Taylor Swift\",\n",
    "        \"text\": \"how to reverse a string in Python?\"\n",
    "    })"
   ],
   "id": "a2ce9789d1c4a5ee",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sure, I can help with that! Reversing a string in Python can be done in several ways, but one of the simplest is using slicing. Here’s a quick example:\\n\\n```python\\noriginal_string = \"Hello, World!\"\\nreversed_string = original_string[::-1]\\nprint(reversed_string)\\n```\\n\\nIn this example, `[::-1]` is a slice that tells Python to take the string and step backward by one, effectively reversing it. It’s a neat little trick! Hope that helps!'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 47
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Langserve\n",
    "See [langchain_with_langserve.py](./langchain_with_langserve.py) for a langchain-powered server\n",
    "\n",
    "You can run it with OPENAI_API_KEY=xxx python langchain_with_langserve.py\n",
    "You can play with you model on http://localhost:8000/chain/playground with UI.\n",
    "\n",
    "Or you can access it programmatically:"
   ],
   "id": "cd76cde2e0b6efb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-03T17:26:04.525995Z",
     "start_time": "2024-06-03T17:26:02.778634Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langserve import RemoteRunnable\n",
    "\n",
    "remote_chain = RemoteRunnable(\"http://localhost:8000/chain\")\n",
    "remote_chain.invoke({\"character\": \"Guido van Rossum\", \"text\": \"How to reverse string in Python?\"})"
   ],
   "id": "344cb5ff8473a13f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You can reverse a string in Python by using slicing. Here\\'s an example:\\n\\n```python\\noriginal_string = \"hello\"\\nreversed_string = original_string[::-1]\\nprint(reversed_string)\\n```\\n\\nThis will output: \"olleh\"'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 48
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### React\n",
    "\n",
    "ReACT is a way to implement \"intelligent\" agent.\n",
    "You give a prompt to LLM like 'You have access to a tool that can frobnicate. When you need to frobnicate to give a better answer you should output json with {\"tool\": \"frobnicate\", \"args\": [\"x\"]}'. When you get this json from LLM you just call some code. In langchain this is done with tools"
   ],
   "id": "9cf205c6f1a97fef"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-03T17:33:48.117889Z",
     "start_time": "2024-06-03T17:33:48.107775Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain.tools import tool\n",
    "\n",
    "\n",
    "@tool\n",
    "def frobnicate(something: str) -> str:\n",
    "    \"\"\"\n",
    "    Frobnicate something.\n",
    "    \"\"\"\n",
    "    return f\"langchain provides very little value over {something}\""
   ],
   "id": "7d776632c515a00f",
   "outputs": [],
   "execution_count": 49
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Let's ask a model to frobnicate without tool:",
   "id": "b8f9e07ebe87f56b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-03T17:36:19.126805Z",
     "start_time": "2024-06-03T17:36:16.585847Z"
    }
   },
   "cell_type": "code",
   "source": "print(chain.invoke({\"character\": \"Batman\", \"text\": \"What's the result of frobnification of 'openai python api'?\"}))",
   "id": "34c8747cabc56e0b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The term \"frobnification\" isn't standard in any of my known databases or in common cryptographic or computational practices. However, it sounds like a whimsical or placeholder term for some form of transformation or encoding process. \n",
      "\n",
      "Without specifics, it's hard to determine the exact outcome. If you've got more details about the algorithm or method behind this \"frobnification,\" I could give you a more precise answer. Until then, stay vigilant, and always question the unknown.\n",
      "\n",
      "I'm Batman.\n"
     ]
    }
   ],
   "execution_count": 51
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The model can't frobnicate out of the box. Let's teach it do frobnicate. ",
   "id": "f9b26cbae01f1cc1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-03T17:39:28.460628Z",
     "start_time": "2024-06-03T17:39:28.457627Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_with_tools = model.bind_tools([frobnicate])\n",
    "chain_with_tools = prompt_template | model_with_tools"
   ],
   "id": "9940ec5d90cd8c9f",
   "outputs": [],
   "execution_count": 56
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-03T21:23:30.719984Z",
     "start_time": "2024-06-03T21:23:29.274699Z"
    }
   },
   "cell_type": "code",
   "source": [
    "response = chain_with_tools.invoke(\n",
    "    {\"character\": \"Batman\", \"text\": \"What's the uppercase result of frobnification of 'openai python api'?\"})"
   ],
   "id": "51a3634ece409f98",
   "outputs": [],
   "execution_count": 69
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-03T21:23:51.266196Z",
     "start_time": "2024-06-03T21:23:51.262932Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(f\"{response.tool_calls=}\")\n",
    "print(f\"{response.content=}\")"
   ],
   "id": "ee725647dec78b68",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response.tool_calls=[{'name': 'frobnicate', 'args': {'something': 'openai python api'}, 'id': 'call_WlWMeh774WeWeOdlvMloL5EF'}]\n",
      "response.content=''\n"
     ]
    }
   ],
   "execution_count": 71
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "`response.tool_calls` were actually not called (notice that the content is empty), we need to create an agent to use tools",
   "id": "f1ad51aece97d70b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-03T17:44:15.476762Z",
     "start_time": "2024-06-03T17:44:15.441426Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "agent = create_react_agent(model_with_tools, [frobnicate])"
   ],
   "id": "8b707ee2ac33be90",
   "outputs": [],
   "execution_count": 62
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-03T21:24:11.109670Z",
     "start_time": "2024-06-03T21:24:07.785635Z"
    }
   },
   "cell_type": "code",
   "source": "response = agent.invoke({\"messages\": \"What's the uppercase result of frobnification of 'openai python api'?\"})",
   "id": "a0a950b7caaa3fc5",
   "outputs": [],
   "execution_count": 73
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-03T21:24:12.266872Z",
     "start_time": "2024-06-03T21:24:12.262707Z"
    }
   },
   "cell_type": "code",
   "source": "print(response[\"messages\"][-1].content)",
   "id": "26aec472b765979c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The result of the frobnication of \"openai python api\" is \"langchain provides very little value over openai python api\". \n",
      "\n",
      "The uppercase version of this result is:\n",
      "\n",
      "\"LANGCHAIN PROVIDES VERY LITTLE VALUE OVER OPENAI PYTHON API\"\n"
     ]
    }
   ],
   "execution_count": 74
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Absolutely correct! This is called Singularity.",
   "id": "c692388c9e4e96b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### RAG\n",
    "\n",
    "RAG (retrieval-augmented generation) allows you to add some extra context to LLM.\n",
    "You have your own private data, that LLM was not trained on.\n",
    "So you can take user query, find part of the documents that are relevant to the query\n",
    "and add this information as part of the context to LLM.\n",
    "\n",
    "So it gives an illusion as if LLM \"knows\" about some of your private data.  \n",
    "\n",
    "Finding relevant documents is usually done with vector DBs by converting text to vectors and calculating e.g. cosine similarity between two vectors"
   ],
   "id": "43cb73c444a03047"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-04T18:44:57.366800Z",
     "start_time": "2024-06-04T18:44:56.939914Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Chroma is a vector DB\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.documents import Document\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "docs = [\n",
    "    Document('My favorite color is blue'),\n",
    "    Document('My favorite movies are Glengarry Glen Ross and Godfather Part 2'),\n",
    "]\n",
    "\n",
    "vectorstore = Chroma.from_documents(docs, embedding=OpenAIEmbeddings())\n",
    "\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(a_doc.page_content for a_doc in docs)\n",
    "\n",
    "\n",
    "system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. If you don't know the answer, say that you \"\n",
    "    \"don't know. Use three sentences maximum and keep the \"\n",
    "    \"answer concise.\"\n",
    "    \"\\n\\n\"\n",
    "    \"Context: {context}\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# return single most relevant document\n",
    "golden_retriever = vectorstore.as_retriever(search_kwargs={'k': 1})\n",
    "\n",
    "after_prompt = {\n",
    "    \"context\": golden_retriever | format_docs,\n",
    "    \"question\": RunnablePassthrough()\n",
    "} | prompt\n",
    "\n",
    "rag_chain =  after_prompt | model | StrOutputParser()\n"
   ],
   "id": "9432530bc8dc3e9d",
   "outputs": [],
   "execution_count": 84
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-04T18:45:02.491823Z",
     "start_time": "2024-06-04T18:45:01.287014Z"
    }
   },
   "cell_type": "code",
   "source": "resp = rag_chain.invoke(\"what is my favorite color?\")",
   "id": "ca4b8c93001efac7",
   "outputs": [],
   "execution_count": 85
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-04T18:45:03.041623Z",
     "start_time": "2024-06-04T18:45:03.038922Z"
    }
   },
   "cell_type": "code",
   "source": "print(resp)",
   "id": "b10bb2064e725bd3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your favorite color is blue.\n"
     ]
    }
   ],
   "execution_count": 86
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Here's a proof that we give LLM only one relevant document in `context`:",
   "id": "f8ed284865646db8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-04T18:45:36.872164Z",
     "start_time": "2024-06-04T18:45:36.656270Z"
    }
   },
   "cell_type": "code",
   "source": "print(after_prompt.invoke(\"what is my favorite color?\").messages[0].content)",
   "id": "88e7663e0f41b704",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, say that you don't know. Use three sentences maximum and keep the answer concise.\n",
      "\n",
      "Context: My favorite color is blue\n"
     ]
    }
   ],
   "execution_count": 90
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-04T18:47:06.399895Z",
     "start_time": "2024-06-04T18:47:04.980991Z"
    }
   },
   "cell_type": "code",
   "source": "print(rag_chain.invoke(\"what actor stars in my favorite movies?\"))",
   "id": "fa465a6291ae8908",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Al Pacino stars in both \"Glengarry Glen Ross\" and \"The Godfather Part II.\"\n"
     ]
    }
   ],
   "execution_count": 93
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Here's a proof that we give LLM only one relevant document in a `context`:",
   "id": "b28753c12f49ad21"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-04T18:47:32.679313Z",
     "start_time": "2024-06-04T18:47:32.411045Z"
    }
   },
   "cell_type": "code",
   "source": "print(after_prompt.invoke(\"what actor stars in my favorite movies?\").messages[0].content)",
   "id": "98d2e7fb996dde64",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, say that you don't know. Use three sentences maximum and keep the answer concise.\n",
      "\n",
      "Context: My favorite movies are Glengarry Glen Ross and Godfather Part 2\n"
     ]
    }
   ],
   "execution_count": 95
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "ad2a1e481ac1e6e9"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
